{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_iris_multi_classfication.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP1fbhQl+AU+z55PqEye/R4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ljm9748/deepstudy/blob/master/03_iris_multi_classfication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOKZUKBMFxG2",
        "colab_type": "code",
        "outputId": "f70c500c-4fcf-493a-bb42-9337122f23c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLxRUzoSFzMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "data=pd.read_csv(r\"/content/gdrive/Shared drives/TNT/스터디/2020년 1학기/NLP/iris.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l3i3SgcGNUM",
        "colab_type": "code",
        "outputId": "ed94448b-4048-47c6-999f-6682ee634d15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-iiP1N9Gpj8",
        "colab_type": "code",
        "outputId": "85a0d77c-384f-4ce2-878a-bf85c48d15a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal.length</th>\n",
              "      <th>sepal.width</th>\n",
              "      <th>petal.length</th>\n",
              "      <th>petal.width</th>\n",
              "      <th>variety</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal.length  sepal.width  petal.length  petal.width variety\n",
              "0           5.1          3.5           1.4          0.2  Setosa\n",
              "1           4.9          3.0           1.4          0.2  Setosa\n",
              "2           4.7          3.2           1.3          0.2  Setosa\n",
              "3           4.6          3.1           1.5          0.2  Setosa\n",
              "4           5.0          3.6           1.4          0.2  Setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Auhh_HfGsEh",
        "colab_type": "code",
        "outputId": "23ad4d9a-0cec-4931-f5a8-b327a2f85f70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "data['variety'].value_counts().plot(kind='bar')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f59c52c0f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEfCAYAAABYu52wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAARHUlEQVR4nO3dfYxldX3H8feHXYyg3QAybrcgLgLR\nkFbArviAtVXUqqBsKkGMD2ul2UarxWhUqvEPk5qATdU+mOpGsPsH8iBooLQ+kEVELQGHB1FACl2h\nQoFdlAdFq7B++8c9I+Myy9yZO3PP/Jj3K5nce373Xu/HOexnzj33d85JVSFJas9ufQeQJM2PBS5J\njbLAJalRFrgkNcoCl6RGrRznm+277761du3acb6lJDXvqquuuqeqJnYeH2uBr127lsnJyXG+pSQ1\nL8ltM427C0WSGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1aqhphEluBX4K7AAerqp1SfYBzgHWArcC\nJ1TVvYsTU5K0s7lsgb+kqg6vqnXd8inAlqo6BNjSLUuSxmSUXSjHAZu7+5uB9aPHkSQNa9gjMQv4\nWpICPlNVm4DVVXVn9/hdwOqZXphkI7AR4IADDhgx7tysPeXfx/p+43brqcf0HWHRuO7a5vobj2EL\n/EVVdUeSpwIXJ/nB9Aerqrpyf5Su7DcBrFu3zsv/SNICGWoXSlXd0d1uA74EHAncnWQNQHe7bbFC\nSpIebdYCT/KkJL8zdR94BfB94EJgQ/e0DcAFixVSkvRow+xCWQ18KcnU8z9fVV9J8h3g3CQnAbcB\nJyxeTEnSzmYt8KraChw2w/iPgaMXI5QkaXYeiSlJjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFL\nUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1\nygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMs\ncElq1NAFnmRFkmuSXNQtH5jkiiS3JDknyRMWL6YkaWdz2QI/Gbhx2vJpwCeq6mDgXuCkhQwmSXps\nQxV4kv2BY4DPdssBXgqc1z1lM7B+MQJKkmY27Bb4J4H3A7/ulp8C3FdVD3fLtwP7zfTCJBuTTCaZ\n3L59+0hhJUmPmLXAkxwLbKuqq+bzBlW1qarWVdW6iYmJ+fxPSJJmsHKI5xwFvDbJq4EnAquAfwD2\nSrKy2wrfH7hj8WJKknY26xZ4Vf1NVe1fVWuBE4FLquqNwNeB47unbQAuWLSUkqRHGWUe+AeA9yS5\nhcE+8dMXJpIkaRjD7EL5jaq6FLi0u78VOHLhI0mShuGRmJLUKAtckhplgUtSoyxwSWqUBS5JjbLA\nJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1yS\nGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalR\nFrgkNcoCl6RGzVrgSZ6Y5Mok301yfZKPdOMHJrkiyS1JzknyhMWPK0maMswW+C+Bl1bVYcDhwCuT\nPB84DfhEVR0M3AuctHgxJUk7m7XAa+Bn3eLu3U8BLwXO68Y3A+sXJaEkaUZD7QNPsiLJtcA24GLg\nv4H7qurh7im3A/stTkRJ0kyGKvCq2lFVhwP7A0cCzxr2DZJsTDKZZHL79u3zjClJ2tmcZqFU1X3A\n14EXAHslWdk9tD9wxy5es6mq1lXVuomJiZHCSpIeMcwslIkke3X39wBeDtzIoMiP7562AbhgsUJK\nkh5t5exPYQ2wOckKBoV/blVdlOQG4OwkfwtcA5y+iDklSTuZtcCr6jrgiBnGtzLYHy5J6oFHYkpS\noyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXK\nApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxw\nSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqNmLfAkT0vy9SQ3JLk+ycnd+D5JLk5y\nc3e79+LHlSRNGWYL/GHgvVV1KPB84K+SHAqcAmypqkOALd2yJGlMZi3wqrqzqq7u7v8UuBHYDzgO\n2Nw9bTOwfrFCSpIebU77wJOsBY4ArgBWV9Wd3UN3Aat38ZqNSSaTTG7fvn2EqJKk6YYu8CRPBs4H\n3l1VD0x/rKoKqJleV1WbqmpdVa2bmJgYKawk6RFDFXiS3RmU95lV9cVu+O4ka7rH1wDbFieiJGkm\nw8xCCXA6cGNVfXzaQxcCG7r7G4ALFj6eJGlXVg7xnKOANwPfS3JtN/ZB4FTg3CQnAbcBJyxOREnS\nTGYt8Kr6FpBdPHz0wsaRJA3LIzElqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLA\nJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1yS\nGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjZq1wJOc\nkWRbku9PG9snycVJbu5u917cmJKknQ2zBf6vwCt3GjsF2FJVhwBbumVJ0hjNWuBVdRnwk52GjwM2\nd/c3A+sXOJckaRbz3Qe+uqru7O7fBaze1ROTbEwymWRy+/bt83w7SdLORv4Ss6oKqMd4fFNVrauq\ndRMTE6O+nSSpM98CvzvJGoDudtvCRZIkDWO+BX4hsKG7vwG4YGHiSJKGNcw0wrOAy4FnJrk9yUnA\nqcDLk9wMvKxbliSN0crZnlBVb9jFQ0cvcBZJ0hx4JKYkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElq\nlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ\n4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUu\nSY2ywCWpURa4JDVqpAJP8sokNyW5JckpCxVKkjS7eRd4khXAp4BXAYcCb0hy6EIFkyQ9tlG2wI8E\nbqmqrVX1K+Bs4LiFiSVJms3KEV67H/Cjacu3A8/b+UlJNgIbu8WfJblphPdc6vYF7hnXm+W0cb3T\nsuC6a9vjff09fabBUQp8KFW1Cdi02O+zFCSZrKp1fefQ3Lnu2rZc198ou1DuAJ42bXn/bkySNAaj\nFPh3gEOSHJjkCcCJwIULE0uSNJt570KpqoeTvBP4KrACOKOqrl+wZG1aFruKHqdcd21blusvVdV3\nBknSPHgkpiQ1ygKXpEZZ4JLUKAtckhq16AfyPF5154K5vqqe1XcWzV2SCeADDM7j88Sp8ap6aW+h\nNDTX34Bb4PNUVTuAm5Ic0HcWzcuZwI3AgcBHgFsZHNugNrj+cBrhSJJcBhwBXAk8ODVeVa/tLZSG\nkuSqqvrDJNdV1bO7se9U1XP7zqbZuf4G3IUymg/3HUDz9lB3e2eSY4D/BfbpMY/mxvWHW+AjS7Ia\nmPqrf2VVbeszj4aT5FjgmwzO5/NPwCrgI1Xl6SAa4PobsMBHkOQE4O+AS4EAfwS8r6rO6zOXpOXB\nLzFH8yHguVW1oarewuAiF+5WaUCSzUn2mra8d5Iz+syk4SX5WJJVSXZPsiXJ9iRv6jvXuFngo9lt\np10mP8bfaSueXVX3TS1U1b0MvpBWG15RVQ8AxzKYgXIw8L5eE/XALzFH85UkXwXO6pZfD/xHj3k0\nvN2S7N0VN0n2wX8PLZlaV8cAX6iq+5P0macX/gc7gqp6X5LXAUd1Q5uq6kt9ZtLQ/h64PMkXGHx/\ncTzw0X4jaQ4uSvID4BfA27sDe/6v50xj55eYWraSHApMHbl3SVXd0GcezU33qen+qtqRZE9gVVXd\n1XeucbLA5yHJT4GZfnEBqqpWjTmShpRkVVU90P3jf5Sq+sm4M2nukuwOvB14cTf0DeDTVfXQrl/1\n+GOBa1lJclFVHZvkh/z2H+GpP77P6Cma5iDJZ4Hdgc3d0JuBHVX1F/2lGj8LfERJDmMw/xvgsqq6\nrs880nKQ5LtVddhsY493TnkbQZKTGZxU56ndz5lJ3tVvKg0ryX5JXpjkxVM/fWfS0HYkOWhqIckz\ngB095umFW+AjSHId8IKqerBbfhJw+dTJdbR0JTmNwbTPG3jkH355IrI2JDka+BywlcHur6cDb6uq\nS3oNNmZOIxxN+O2/+ju6MS1964FnVtUv+w6iefkWcAjwzG75ph6z9MYCH83ngCuSTM39Xg+c3mMe\nDW8rgy/BLPA2XV5VzwF+851TkquB5/QXafws8BFU1ceTXAq8qBv686q6psdIGt7PgWuTbGFaiVfV\nX/cXSbNJ8rvAfsAeSY7gkU+8q4A9ewvWEwt8BEmez+Cyald3y6uSPK+qrug5mmZ3Yfejtvwp8FZg\nf+Dj08YfAD7YR6A++SXmCJJcAzynul9ikt2Aye6jnaRFkuR1VXV+3zn65hb4aFLT/gJW1a+T+Dtd\nwpKcW1UnJPkeMxxN6wyiZnw7yenA71XVq7rTIrygqpbVd1BugY8gyRcZXMzhX7qhdwAvqar1vYXS\nY0qypqruTPL0mR6vqtvGnUlzl+TLDCYRfKiqDus2nK6pqj/oOdpYWeAjSPJU4B8ZnBCpgC3Au72s\nmrS4pi5gnOSaqjqiG7u2qg7vO9s4+XF/BF1Rn9h3Ds3dLk5Idj8wCby3qraOP5Xm4MEkT6Fbh92E\ngvv7jTR+Fvg8JHl/VX0syT8x835Up6ItfZ8Ebgc+z2Aq2onAQcDVwBnAn/SWTMN4D4NZRAcl+TYw\nweCc7suKBT4/N3a3k72m0Cheu9OJjzZ1H8E/kGTZTUdrRZLnAj+qqquT/DHwl8DrgK8x+IO8rFjg\n81BV/9bdTp3KcmoK4ZO76/Rp6ft5khOA87rl43nkii5+MbR0fQZ4WXf/hQwuLP4u4HBgE8tsK9yz\nEY4gyee7g3eeBHwfuCHJsruwaqPeyOAc0tuAu7v7b0qyB/DOPoPpMa2YdtGN1zO4jOH5VfVhBhc2\nXlYs8NEc2m1xrwe+DBzIoAi0hCVZAbyjql5TVftW1UR3/5aq+kVVfavvjNqlFdOOtTgamH72wWW3\nR2HZ/R9eYLt3l3ZaD/xzVT2UxI/fS1x3DcUXzf5MLUFnAd9Icg+DCxp/EyDJwTgLRXP0aeBW4LvA\nZd3BIe4Db8M1SS4EvgA8ODVYVV/sL5JmU1Uf7U5Atgb42rQjoXdjsC98WfFAnnnqvrQ8vqrOnTYW\nBvvoHu4vmYaR5HMzDFdVvW3sYaR5ssBHkGSyqtb1nUPS8mSBjyDJqcA9wDn89sfwn+zyReqVB2Hp\n8cQCH0GSH84wXFX1jLGH0VCSbAXewmDK2UwFvvlRL5KWKAtcy0qSdzOYP7wGOBc4y6soqVUW+AiS\n7MngnAwHVNXGJIcwuFDuRT1H0yy6GUMndj97MDgnyllVdXOvwaQ5sMBHkOQc4CrgLVX1+12h/+dy\nO6Vl67prK54BPLuqVvSdRxqWR2KO5qCq+hjwEEBV/ZxHLrKqJSzJyiSvSXImg6NobwL+rOdY0px4\nIM9oftWdO2PqnMQHMe0K51p6krwceAPwauBK4GxgY1U9+JgvlJYgd6HMQ5JPMTikd08GZ0M7lMHp\nLI8C3lpVl/aXTo8lySUM9nefX1X39p1HGoUFPg9JTmbw5dca4GLgfxhcCOCKqrqnz2ySlg8LfAS7\nmMlwdlX9V6/BJC0LFvgCcSaDpHFzFsoInMkgqU9ugc/DLmYyXOBMBknjZIHPgzMZJC0FFrgkNcp9\n4JLUKAtckhplgUtSoyxwSWrU/wOMZrpyY6azHgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s49P6MNjHs1g",
        "colab_type": "code",
        "outputId": "a518fa3c-dbf2-4101-fab9-35b94fc62a14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "data['variety'] = data['variety'].replace(['Virginica','Setosa','Versicolor'],[0,1,2])\n",
        "# Iris-virginica는 0, Iris-setosa는 1, Iris-versicolor는 2가 됨.\n",
        "data['variety'].value_counts().plot(kind='bar')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f59b97e9f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD1CAYAAABJE67gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASZi36nvHBor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "data_x= data[['sepal.length', 'sepal.width', 'petal.length', 'petal.width']].values\n",
        "data_y= data['variety'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M__BF9hUHiOG",
        "colab_type": "code",
        "outputId": "b6aa9303-9ea5-40d8-c5f5-fb0c8b16f348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "data_x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2],\n",
              "       [5.4, 3.9, 1.7, 0.4],\n",
              "       [4.6, 3.4, 1.4, 0.3],\n",
              "       [5. , 3.4, 1.5, 0.2],\n",
              "       [4.4, 2.9, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.1],\n",
              "       [5.4, 3.7, 1.5, 0.2],\n",
              "       [4.8, 3.4, 1.6, 0.2],\n",
              "       [4.8, 3. , 1.4, 0.1],\n",
              "       [4.3, 3. , 1.1, 0.1],\n",
              "       [5.8, 4. , 1.2, 0.2],\n",
              "       [5.7, 4.4, 1.5, 0.4],\n",
              "       [5.4, 3.9, 1.3, 0.4],\n",
              "       [5.1, 3.5, 1.4, 0.3],\n",
              "       [5.7, 3.8, 1.7, 0.3],\n",
              "       [5.1, 3.8, 1.5, 0.3],\n",
              "       [5.4, 3.4, 1.7, 0.2],\n",
              "       [5.1, 3.7, 1.5, 0.4],\n",
              "       [4.6, 3.6, 1. , 0.2],\n",
              "       [5.1, 3.3, 1.7, 0.5],\n",
              "       [4.8, 3.4, 1.9, 0.2],\n",
              "       [5. , 3. , 1.6, 0.2],\n",
              "       [5. , 3.4, 1.6, 0.4],\n",
              "       [5.2, 3.5, 1.5, 0.2],\n",
              "       [5.2, 3.4, 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.6, 0.2],\n",
              "       [4.8, 3.1, 1.6, 0.2],\n",
              "       [5.4, 3.4, 1.5, 0.4],\n",
              "       [5.2, 4.1, 1.5, 0.1],\n",
              "       [5.5, 4.2, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.2, 1.2, 0.2],\n",
              "       [5.5, 3.5, 1.3, 0.2],\n",
              "       [4.9, 3.6, 1.4, 0.1],\n",
              "       [4.4, 3. , 1.3, 0.2],\n",
              "       [5.1, 3.4, 1.5, 0.2],\n",
              "       [5. , 3.5, 1.3, 0.3],\n",
              "       [4.5, 2.3, 1.3, 0.3],\n",
              "       [4.4, 3.2, 1.3, 0.2],\n",
              "       [5. , 3.5, 1.6, 0.6],\n",
              "       [5.1, 3.8, 1.9, 0.4],\n",
              "       [4.8, 3. , 1.4, 0.3],\n",
              "       [5.1, 3.8, 1.6, 0.2],\n",
              "       [4.6, 3.2, 1.4, 0.2],\n",
              "       [5.3, 3.7, 1.5, 0.2],\n",
              "       [5. , 3.3, 1.4, 0.2],\n",
              "       [7. , 3.2, 4.7, 1.4],\n",
              "       [6.4, 3.2, 4.5, 1.5],\n",
              "       [6.9, 3.1, 4.9, 1.5],\n",
              "       [5.5, 2.3, 4. , 1.3],\n",
              "       [6.5, 2.8, 4.6, 1.5],\n",
              "       [5.7, 2.8, 4.5, 1.3],\n",
              "       [6.3, 3.3, 4.7, 1.6],\n",
              "       [4.9, 2.4, 3.3, 1. ],\n",
              "       [6.6, 2.9, 4.6, 1.3],\n",
              "       [5.2, 2.7, 3.9, 1.4],\n",
              "       [5. , 2. , 3.5, 1. ],\n",
              "       [5.9, 3. , 4.2, 1.5],\n",
              "       [6. , 2.2, 4. , 1. ],\n",
              "       [6.1, 2.9, 4.7, 1.4],\n",
              "       [5.6, 2.9, 3.6, 1.3],\n",
              "       [6.7, 3.1, 4.4, 1.4],\n",
              "       [5.6, 3. , 4.5, 1.5],\n",
              "       [5.8, 2.7, 4.1, 1. ],\n",
              "       [6.2, 2.2, 4.5, 1.5],\n",
              "       [5.6, 2.5, 3.9, 1.1],\n",
              "       [5.9, 3.2, 4.8, 1.8],\n",
              "       [6.1, 2.8, 4. , 1.3],\n",
              "       [6.3, 2.5, 4.9, 1.5],\n",
              "       [6.1, 2.8, 4.7, 1.2],\n",
              "       [6.4, 2.9, 4.3, 1.3],\n",
              "       [6.6, 3. , 4.4, 1.4],\n",
              "       [6.8, 2.8, 4.8, 1.4],\n",
              "       [6.7, 3. , 5. , 1.7],\n",
              "       [6. , 2.9, 4.5, 1.5],\n",
              "       [5.7, 2.6, 3.5, 1. ],\n",
              "       [5.5, 2.4, 3.8, 1.1],\n",
              "       [5.5, 2.4, 3.7, 1. ],\n",
              "       [5.8, 2.7, 3.9, 1.2],\n",
              "       [6. , 2.7, 5.1, 1.6],\n",
              "       [5.4, 3. , 4.5, 1.5],\n",
              "       [6. , 3.4, 4.5, 1.6],\n",
              "       [6.7, 3.1, 4.7, 1.5],\n",
              "       [6.3, 2.3, 4.4, 1.3],\n",
              "       [5.6, 3. , 4.1, 1.3],\n",
              "       [5.5, 2.5, 4. , 1.3],\n",
              "       [5.5, 2.6, 4.4, 1.2],\n",
              "       [6.1, 3. , 4.6, 1.4],\n",
              "       [5.8, 2.6, 4. , 1.2],\n",
              "       [5. , 2.3, 3.3, 1. ],\n",
              "       [5.6, 2.7, 4.2, 1.3],\n",
              "       [5.7, 3. , 4.2, 1.2],\n",
              "       [5.7, 2.9, 4.2, 1.3],\n",
              "       [6.2, 2.9, 4.3, 1.3],\n",
              "       [5.1, 2.5, 3. , 1.1],\n",
              "       [5.7, 2.8, 4.1, 1.3],\n",
              "       [6.3, 3.3, 6. , 2.5],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [7.1, 3. , 5.9, 2.1],\n",
              "       [6.3, 2.9, 5.6, 1.8],\n",
              "       [6.5, 3. , 5.8, 2.2],\n",
              "       [7.6, 3. , 6.6, 2.1],\n",
              "       [4.9, 2.5, 4.5, 1.7],\n",
              "       [7.3, 2.9, 6.3, 1.8],\n",
              "       [6.7, 2.5, 5.8, 1.8],\n",
              "       [7.2, 3.6, 6.1, 2.5],\n",
              "       [6.5, 3.2, 5.1, 2. ],\n",
              "       [6.4, 2.7, 5.3, 1.9],\n",
              "       [6.8, 3. , 5.5, 2.1],\n",
              "       [5.7, 2.5, 5. , 2. ],\n",
              "       [5.8, 2.8, 5.1, 2.4],\n",
              "       [6.4, 3.2, 5.3, 2.3],\n",
              "       [6.5, 3. , 5.5, 1.8],\n",
              "       [7.7, 3.8, 6.7, 2.2],\n",
              "       [7.7, 2.6, 6.9, 2.3],\n",
              "       [6. , 2.2, 5. , 1.5],\n",
              "       [6.9, 3.2, 5.7, 2.3],\n",
              "       [5.6, 2.8, 4.9, 2. ],\n",
              "       [7.7, 2.8, 6.7, 2. ],\n",
              "       [6.3, 2.7, 4.9, 1.8],\n",
              "       [6.7, 3.3, 5.7, 2.1],\n",
              "       [7.2, 3.2, 6. , 1.8],\n",
              "       [6.2, 2.8, 4.8, 1.8],\n",
              "       [6.1, 3. , 4.9, 1.8],\n",
              "       [6.4, 2.8, 5.6, 2.1],\n",
              "       [7.2, 3. , 5.8, 1.6],\n",
              "       [7.4, 2.8, 6.1, 1.9],\n",
              "       [7.9, 3.8, 6.4, 2. ],\n",
              "       [6.4, 2.8, 5.6, 2.2],\n",
              "       [6.3, 2.8, 5.1, 1.5],\n",
              "       [6.1, 2.6, 5.6, 1.4],\n",
              "       [7.7, 3. , 6.1, 2.3],\n",
              "       [6.3, 3.4, 5.6, 2.4],\n",
              "       [6.4, 3.1, 5.5, 1.8],\n",
              "       [6. , 3. , 4.8, 1.8],\n",
              "       [6.9, 3.1, 5.4, 2.1],\n",
              "       [6.7, 3.1, 5.6, 2.4],\n",
              "       [6.9, 3.1, 5.1, 2.3],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [6.8, 3.2, 5.9, 2.3],\n",
              "       [6.7, 3.3, 5.7, 2.5],\n",
              "       [6.7, 3. , 5.2, 2.3],\n",
              "       [6.3, 2.5, 5. , 1.9],\n",
              "       [6.5, 3. , 5.2, 2. ],\n",
              "       [6.2, 3.4, 5.4, 2.3],\n",
              "       [5.9, 3. , 5.1, 1.8]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsedNucNHjrm",
        "colab_type": "code",
        "outputId": "2b42ef98-8a76-4b91-d936-fc4b692e6c48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "data_y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEy5EyeeJePD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, X_test, y_train, y_test)= train_test_split(data_x, data_y, train_size=0.8, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4dY_0-FJzMl",
        "colab_type": "code",
        "outputId": "606e0ae8-dbc4-4c07-c66d-7af410836743",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train= to_categorical(y_train)\n",
        "y_test= to_categorical(y_test)\n",
        "\n",
        "print(y_train)\n",
        "print(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]]\n",
            "[[0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6flJZZ2HnO-",
        "colab_type": "code",
        "outputId": "48b98b48-5b05-4ce4-d8bb-47043b0874b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Dense(3, input_dim=4, activation='softmax'))\n",
        "sgd=optimizers.SGD(lr=0.01)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history=model.fit(X_train, y_train, batch_size=1, epochs=200, validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 120 samples, validate on 30 samples\n",
            "Epoch 1/200\n",
            "120/120 [==============================] - 0s 3ms/sample - loss: 4.2377 - acc: 0.3083 - val_loss: 3.4037 - val_acc: 0.4333\n",
            "Epoch 2/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 2.7733 - acc: 0.3167 - val_loss: 2.4920 - val_acc: 0.4333\n",
            "Epoch 3/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 2.0310 - acc: 0.4583 - val_loss: 2.1992 - val_acc: 0.4667\n",
            "Epoch 4/200\n",
            "120/120 [==============================] - 0s 988us/sample - loss: 1.8012 - acc: 0.5083 - val_loss: 2.0766 - val_acc: 0.3333\n",
            "Epoch 5/200\n",
            "120/120 [==============================] - 0s 945us/sample - loss: 1.6869 - acc: 0.4583 - val_loss: 1.9568 - val_acc: 0.3000\n",
            "Epoch 6/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 1.5881 - acc: 0.4833 - val_loss: 1.8737 - val_acc: 0.2000\n",
            "Epoch 7/200\n",
            "120/120 [==============================] - 0s 975us/sample - loss: 1.4957 - acc: 0.4167 - val_loss: 1.7490 - val_acc: 0.3000\n",
            "Epoch 8/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 1.4089 - acc: 0.4417 - val_loss: 1.6506 - val_acc: 0.3000\n",
            "Epoch 9/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 1.3203 - acc: 0.4750 - val_loss: 1.5532 - val_acc: 0.3000\n",
            "Epoch 10/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 1.2448 - acc: 0.4500 - val_loss: 1.4458 - val_acc: 0.3000\n",
            "Epoch 11/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 1.1666 - acc: 0.4750 - val_loss: 1.3702 - val_acc: 0.3000\n",
            "Epoch 12/200\n",
            "120/120 [==============================] - 0s 955us/sample - loss: 1.0985 - acc: 0.4750 - val_loss: 1.2876 - val_acc: 0.3000\n",
            "Epoch 13/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 1.0303 - acc: 0.4750 - val_loss: 1.1992 - val_acc: 0.3333\n",
            "Epoch 14/200\n",
            "120/120 [==============================] - 0s 999us/sample - loss: 0.9707 - acc: 0.5083 - val_loss: 1.1486 - val_acc: 0.3000\n",
            "Epoch 15/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.9116 - acc: 0.5250 - val_loss: 1.0802 - val_acc: 0.3000\n",
            "Epoch 16/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.8636 - acc: 0.5333 - val_loss: 1.0286 - val_acc: 0.3000\n",
            "Epoch 17/200\n",
            "120/120 [==============================] - 0s 978us/sample - loss: 0.8181 - acc: 0.5417 - val_loss: 0.9818 - val_acc: 0.3667\n",
            "Epoch 18/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.7807 - acc: 0.7083 - val_loss: 0.9305 - val_acc: 0.4333\n",
            "Epoch 19/200\n",
            "120/120 [==============================] - 0s 997us/sample - loss: 0.7433 - acc: 0.7167 - val_loss: 0.8820 - val_acc: 0.6000\n",
            "Epoch 20/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.7098 - acc: 0.8333 - val_loss: 0.8421 - val_acc: 0.6667\n",
            "Epoch 21/200\n",
            "120/120 [==============================] - 0s 983us/sample - loss: 0.6796 - acc: 0.8667 - val_loss: 0.8049 - val_acc: 0.7000\n",
            "Epoch 22/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.6544 - acc: 0.8583 - val_loss: 0.7907 - val_acc: 0.7000\n",
            "Epoch 23/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.6318 - acc: 0.8667 - val_loss: 0.7606 - val_acc: 0.7000\n",
            "Epoch 24/200\n",
            "120/120 [==============================] - 0s 995us/sample - loss: 0.6102 - acc: 0.8583 - val_loss: 0.7265 - val_acc: 0.7000\n",
            "Epoch 25/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.5896 - acc: 0.8417 - val_loss: 0.7012 - val_acc: 0.7000\n",
            "Epoch 26/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.5711 - acc: 0.8500 - val_loss: 0.6771 - val_acc: 0.7000\n",
            "Epoch 27/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.5576 - acc: 0.8583 - val_loss: 0.6735 - val_acc: 0.7000\n",
            "Epoch 28/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.5407 - acc: 0.8667 - val_loss: 0.6491 - val_acc: 0.7000\n",
            "Epoch 29/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.5275 - acc: 0.8333 - val_loss: 0.6417 - val_acc: 0.7000\n",
            "Epoch 30/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.5159 - acc: 0.8667 - val_loss: 0.6437 - val_acc: 0.7000\n",
            "Epoch 31/200\n",
            "120/120 [==============================] - 0s 971us/sample - loss: 0.5046 - acc: 0.8333 - val_loss: 0.6030 - val_acc: 0.7000\n",
            "Epoch 32/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.4922 - acc: 0.8500 - val_loss: 0.5913 - val_acc: 0.7000\n",
            "Epoch 33/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.4819 - acc: 0.8583 - val_loss: 0.5956 - val_acc: 0.7000\n",
            "Epoch 34/200\n",
            "120/120 [==============================] - 0s 989us/sample - loss: 0.4759 - acc: 0.8833 - val_loss: 0.5620 - val_acc: 0.7667\n",
            "Epoch 35/200\n",
            "120/120 [==============================] - 0s 956us/sample - loss: 0.4635 - acc: 0.8667 - val_loss: 0.5623 - val_acc: 0.7000\n",
            "Epoch 36/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.4559 - acc: 0.8667 - val_loss: 0.5556 - val_acc: 0.7000\n",
            "Epoch 37/200\n",
            "120/120 [==============================] - 0s 973us/sample - loss: 0.4498 - acc: 0.8667 - val_loss: 0.5366 - val_acc: 0.7667\n",
            "Epoch 38/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.4399 - acc: 0.8833 - val_loss: 0.5319 - val_acc: 0.7667\n",
            "Epoch 39/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.4369 - acc: 0.8667 - val_loss: 0.5312 - val_acc: 0.7000\n",
            "Epoch 40/200\n",
            "120/120 [==============================] - 0s 956us/sample - loss: 0.4293 - acc: 0.8583 - val_loss: 0.5181 - val_acc: 0.7667\n",
            "Epoch 41/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.4230 - acc: 0.8667 - val_loss: 0.4901 - val_acc: 0.8667\n",
            "Epoch 42/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.4177 - acc: 0.9000 - val_loss: 0.5130 - val_acc: 0.7000\n",
            "Epoch 43/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.4096 - acc: 0.8750 - val_loss: 0.5048 - val_acc: 0.7333\n",
            "Epoch 44/200\n",
            "120/120 [==============================] - 0s 994us/sample - loss: 0.4056 - acc: 0.9000 - val_loss: 0.4878 - val_acc: 0.8333\n",
            "Epoch 45/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.4024 - acc: 0.8833 - val_loss: 0.4920 - val_acc: 0.7667\n",
            "Epoch 46/200\n",
            "120/120 [==============================] - 0s 977us/sample - loss: 0.3955 - acc: 0.8833 - val_loss: 0.4774 - val_acc: 0.8333\n",
            "Epoch 47/200\n",
            "120/120 [==============================] - 0s 994us/sample - loss: 0.3925 - acc: 0.9000 - val_loss: 0.4668 - val_acc: 0.8333\n",
            "Epoch 48/200\n",
            "120/120 [==============================] - 0s 982us/sample - loss: 0.3850 - acc: 0.9000 - val_loss: 0.4715 - val_acc: 0.8000\n",
            "Epoch 49/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.3818 - acc: 0.8917 - val_loss: 0.4512 - val_acc: 0.8667\n",
            "Epoch 50/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.3776 - acc: 0.8917 - val_loss: 0.4575 - val_acc: 0.8333\n",
            "Epoch 51/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.3718 - acc: 0.9083 - val_loss: 0.4686 - val_acc: 0.7667\n",
            "Epoch 52/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.3695 - acc: 0.9083 - val_loss: 0.4489 - val_acc: 0.8333\n",
            "Epoch 53/200\n",
            "120/120 [==============================] - 0s 983us/sample - loss: 0.3655 - acc: 0.8917 - val_loss: 0.4405 - val_acc: 0.8333\n",
            "Epoch 54/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.3608 - acc: 0.9000 - val_loss: 0.4428 - val_acc: 0.8333\n",
            "Epoch 55/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.3575 - acc: 0.9083 - val_loss: 0.4522 - val_acc: 0.7667\n",
            "Epoch 56/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.3561 - acc: 0.8917 - val_loss: 0.4325 - val_acc: 0.8333\n",
            "Epoch 57/200\n",
            "120/120 [==============================] - 0s 969us/sample - loss: 0.3507 - acc: 0.9333 - val_loss: 0.4339 - val_acc: 0.8333\n",
            "Epoch 58/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.3486 - acc: 0.9000 - val_loss: 0.4372 - val_acc: 0.8333\n",
            "Epoch 59/200\n",
            "120/120 [==============================] - 0s 998us/sample - loss: 0.3499 - acc: 0.8917 - val_loss: 0.4238 - val_acc: 0.8333\n",
            "Epoch 60/200\n",
            "120/120 [==============================] - 0s 990us/sample - loss: 0.3393 - acc: 0.9167 - val_loss: 0.4179 - val_acc: 0.8333\n",
            "Epoch 61/200\n",
            "120/120 [==============================] - 0s 970us/sample - loss: 0.3385 - acc: 0.9167 - val_loss: 0.4129 - val_acc: 0.8333\n",
            "Epoch 62/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.3346 - acc: 0.9083 - val_loss: 0.3978 - val_acc: 0.9000\n",
            "Epoch 63/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.3322 - acc: 0.9167 - val_loss: 0.4093 - val_acc: 0.8333\n",
            "Epoch 64/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.3291 - acc: 0.9167 - val_loss: 0.4067 - val_acc: 0.8333\n",
            "Epoch 65/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.3262 - acc: 0.9333 - val_loss: 0.4083 - val_acc: 0.8333\n",
            "Epoch 66/200\n",
            "120/120 [==============================] - 0s 972us/sample - loss: 0.3226 - acc: 0.9167 - val_loss: 0.4001 - val_acc: 0.8333\n",
            "Epoch 67/200\n",
            "120/120 [==============================] - 0s 994us/sample - loss: 0.3242 - acc: 0.9167 - val_loss: 0.3788 - val_acc: 0.9000\n",
            "Epoch 68/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.3194 - acc: 0.9333 - val_loss: 0.3784 - val_acc: 0.9000\n",
            "Epoch 69/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.3153 - acc: 0.9167 - val_loss: 0.3939 - val_acc: 0.8333\n",
            "Epoch 70/200\n",
            "120/120 [==============================] - 0s 989us/sample - loss: 0.3159 - acc: 0.9250 - val_loss: 0.3828 - val_acc: 0.8667\n",
            "Epoch 71/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.3111 - acc: 0.9250 - val_loss: 0.3801 - val_acc: 0.8667\n",
            "Epoch 72/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.3080 - acc: 0.9333 - val_loss: 0.3812 - val_acc: 0.8667\n",
            "Epoch 73/200\n",
            "120/120 [==============================] - 0s 963us/sample - loss: 0.3050 - acc: 0.9333 - val_loss: 0.3833 - val_acc: 0.8333\n",
            "Epoch 74/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.3035 - acc: 0.9250 - val_loss: 0.3807 - val_acc: 0.8333\n",
            "Epoch 75/200\n",
            "120/120 [==============================] - 0s 969us/sample - loss: 0.3021 - acc: 0.9167 - val_loss: 0.3730 - val_acc: 0.8667\n",
            "Epoch 76/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2997 - acc: 0.9417 - val_loss: 0.3671 - val_acc: 0.9000\n",
            "Epoch 77/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2990 - acc: 0.9250 - val_loss: 0.3637 - val_acc: 0.9000\n",
            "Epoch 78/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2943 - acc: 0.9333 - val_loss: 0.3694 - val_acc: 0.8333\n",
            "Epoch 79/200\n",
            "120/120 [==============================] - 0s 989us/sample - loss: 0.2923 - acc: 0.9333 - val_loss: 0.3624 - val_acc: 0.9000\n",
            "Epoch 80/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2906 - acc: 0.9417 - val_loss: 0.3711 - val_acc: 0.8333\n",
            "Epoch 81/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2873 - acc: 0.9333 - val_loss: 0.3612 - val_acc: 0.8667\n",
            "Epoch 82/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2858 - acc: 0.9500 - val_loss: 0.3573 - val_acc: 0.9000\n",
            "Epoch 83/200\n",
            "120/120 [==============================] - 0s 979us/sample - loss: 0.2869 - acc: 0.9417 - val_loss: 0.3481 - val_acc: 0.9000\n",
            "Epoch 84/200\n",
            "120/120 [==============================] - 0s 992us/sample - loss: 0.2824 - acc: 0.9333 - val_loss: 0.3614 - val_acc: 0.8333\n",
            "Epoch 85/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2825 - acc: 0.9417 - val_loss: 0.3634 - val_acc: 0.8333\n",
            "Epoch 86/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2784 - acc: 0.9333 - val_loss: 0.3494 - val_acc: 0.9000\n",
            "Epoch 87/200\n",
            "120/120 [==============================] - 0s 973us/sample - loss: 0.2762 - acc: 0.9333 - val_loss: 0.3601 - val_acc: 0.8333\n",
            "Epoch 88/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2782 - acc: 0.9500 - val_loss: 0.3362 - val_acc: 0.9000\n",
            "Epoch 89/200\n",
            "120/120 [==============================] - 0s 991us/sample - loss: 0.2723 - acc: 0.9500 - val_loss: 0.3360 - val_acc: 0.9000\n",
            "Epoch 90/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2729 - acc: 0.9333 - val_loss: 0.3437 - val_acc: 0.9000\n",
            "Epoch 91/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2694 - acc: 0.9500 - val_loss: 0.3414 - val_acc: 0.9000\n",
            "Epoch 92/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2677 - acc: 0.9417 - val_loss: 0.3357 - val_acc: 0.9000\n",
            "Epoch 93/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2670 - acc: 0.9417 - val_loss: 0.3258 - val_acc: 0.9333\n",
            "Epoch 94/200\n",
            "120/120 [==============================] - 0s 971us/sample - loss: 0.2637 - acc: 0.9417 - val_loss: 0.3338 - val_acc: 0.9000\n",
            "Epoch 95/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2636 - acc: 0.9417 - val_loss: 0.3273 - val_acc: 0.9000\n",
            "Epoch 96/200\n",
            "120/120 [==============================] - 0s 971us/sample - loss: 0.2604 - acc: 0.9417 - val_loss: 0.3557 - val_acc: 0.8333\n",
            "Epoch 97/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2612 - acc: 0.9500 - val_loss: 0.3192 - val_acc: 0.9333\n",
            "Epoch 98/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2592 - acc: 0.9417 - val_loss: 0.3222 - val_acc: 0.9000\n",
            "Epoch 99/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2560 - acc: 0.9333 - val_loss: 0.3330 - val_acc: 0.8667\n",
            "Epoch 100/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2565 - acc: 0.9583 - val_loss: 0.3302 - val_acc: 0.8667\n",
            "Epoch 101/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2526 - acc: 0.9583 - val_loss: 0.3179 - val_acc: 0.9000\n",
            "Epoch 102/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2512 - acc: 0.9500 - val_loss: 0.3140 - val_acc: 0.9333\n",
            "Epoch 103/200\n",
            "120/120 [==============================] - 0s 991us/sample - loss: 0.2488 - acc: 0.9583 - val_loss: 0.3178 - val_acc: 0.9000\n",
            "Epoch 104/200\n",
            "120/120 [==============================] - 0s 966us/sample - loss: 0.2516 - acc: 0.9417 - val_loss: 0.3063 - val_acc: 0.9333\n",
            "Epoch 105/200\n",
            "120/120 [==============================] - 0s 973us/sample - loss: 0.2463 - acc: 0.9500 - val_loss: 0.3098 - val_acc: 0.9333\n",
            "Epoch 106/200\n",
            "120/120 [==============================] - 0s 977us/sample - loss: 0.2459 - acc: 0.9417 - val_loss: 0.3067 - val_acc: 0.9333\n",
            "Epoch 107/200\n",
            "120/120 [==============================] - 0s 991us/sample - loss: 0.2425 - acc: 0.9500 - val_loss: 0.3208 - val_acc: 0.8667\n",
            "Epoch 108/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2428 - acc: 0.9500 - val_loss: 0.3205 - val_acc: 0.8667\n",
            "Epoch 109/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2383 - acc: 0.9583 - val_loss: 0.2964 - val_acc: 0.9333\n",
            "Epoch 110/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2395 - acc: 0.9417 - val_loss: 0.3060 - val_acc: 0.9333\n",
            "Epoch 111/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2374 - acc: 0.9417 - val_loss: 0.3093 - val_acc: 0.9000\n",
            "Epoch 112/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2365 - acc: 0.9500 - val_loss: 0.3062 - val_acc: 0.9000\n",
            "Epoch 113/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2351 - acc: 0.9583 - val_loss: 0.2944 - val_acc: 0.9333\n",
            "Epoch 114/200\n",
            "120/120 [==============================] - 0s 991us/sample - loss: 0.2344 - acc: 0.9500 - val_loss: 0.2899 - val_acc: 0.9333\n",
            "Epoch 115/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2325 - acc: 0.9500 - val_loss: 0.2957 - val_acc: 0.9333\n",
            "Epoch 116/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2308 - acc: 0.9583 - val_loss: 0.3122 - val_acc: 0.8667\n",
            "Epoch 117/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2309 - acc: 0.9583 - val_loss: 0.3092 - val_acc: 0.8667\n",
            "Epoch 118/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2291 - acc: 0.9500 - val_loss: 0.2992 - val_acc: 0.9333\n",
            "Epoch 119/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2277 - acc: 0.9500 - val_loss: 0.2932 - val_acc: 0.9333\n",
            "Epoch 120/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2261 - acc: 0.9500 - val_loss: 0.2892 - val_acc: 0.9333\n",
            "Epoch 121/200\n",
            "120/120 [==============================] - 0s 997us/sample - loss: 0.2250 - acc: 0.9500 - val_loss: 0.2856 - val_acc: 0.9333\n",
            "Epoch 122/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2233 - acc: 0.9500 - val_loss: 0.2891 - val_acc: 0.9333\n",
            "Epoch 123/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2221 - acc: 0.9500 - val_loss: 0.2840 - val_acc: 0.9333\n",
            "Epoch 124/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2222 - acc: 0.9500 - val_loss: 0.2822 - val_acc: 0.9333\n",
            "Epoch 125/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2197 - acc: 0.9500 - val_loss: 0.2786 - val_acc: 0.9333\n",
            "Epoch 126/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2186 - acc: 0.9500 - val_loss: 0.2841 - val_acc: 0.9333\n",
            "Epoch 127/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2170 - acc: 0.9500 - val_loss: 0.2772 - val_acc: 0.9333\n",
            "Epoch 128/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2157 - acc: 0.9583 - val_loss: 0.2899 - val_acc: 0.9333\n",
            "Epoch 129/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2154 - acc: 0.9583 - val_loss: 0.2773 - val_acc: 0.9333\n",
            "Epoch 130/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2142 - acc: 0.9583 - val_loss: 0.2801 - val_acc: 0.9333\n",
            "Epoch 131/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2146 - acc: 0.9500 - val_loss: 0.2660 - val_acc: 0.9333\n",
            "Epoch 132/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2125 - acc: 0.9583 - val_loss: 0.2693 - val_acc: 0.9333\n",
            "Epoch 133/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2107 - acc: 0.9667 - val_loss: 0.2901 - val_acc: 0.8667\n",
            "Epoch 134/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2112 - acc: 0.9500 - val_loss: 0.2783 - val_acc: 0.9333\n",
            "Epoch 135/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2096 - acc: 0.9583 - val_loss: 0.2673 - val_acc: 0.9333\n",
            "Epoch 136/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2087 - acc: 0.9583 - val_loss: 0.2796 - val_acc: 0.9333\n",
            "Epoch 137/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2117 - acc: 0.9583 - val_loss: 0.2746 - val_acc: 0.9333\n",
            "Epoch 138/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2060 - acc: 0.9583 - val_loss: 0.2805 - val_acc: 0.9000\n",
            "Epoch 139/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2055 - acc: 0.9750 - val_loss: 0.2789 - val_acc: 0.9000\n",
            "Epoch 140/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2054 - acc: 0.9583 - val_loss: 0.2681 - val_acc: 0.9333\n",
            "Epoch 141/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2022 - acc: 0.9667 - val_loss: 0.2741 - val_acc: 0.9333\n",
            "Epoch 142/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2035 - acc: 0.9583 - val_loss: 0.2723 - val_acc: 0.9333\n",
            "Epoch 143/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2004 - acc: 0.9583 - val_loss: 0.2569 - val_acc: 0.9333\n",
            "Epoch 144/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.2006 - acc: 0.9667 - val_loss: 0.2488 - val_acc: 0.9333\n",
            "Epoch 145/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1980 - acc: 0.9583 - val_loss: 0.2672 - val_acc: 0.9333\n",
            "Epoch 146/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1993 - acc: 0.9667 - val_loss: 0.2684 - val_acc: 0.9333\n",
            "Epoch 147/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1973 - acc: 0.9667 - val_loss: 0.2559 - val_acc: 0.9333\n",
            "Epoch 148/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1975 - acc: 0.9583 - val_loss: 0.2537 - val_acc: 0.9333\n",
            "Epoch 149/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1955 - acc: 0.9583 - val_loss: 0.2483 - val_acc: 0.9333\n",
            "Epoch 150/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1937 - acc: 0.9667 - val_loss: 0.2501 - val_acc: 0.9333\n",
            "Epoch 151/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1934 - acc: 0.9667 - val_loss: 0.2540 - val_acc: 0.9333\n",
            "Epoch 152/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1933 - acc: 0.9667 - val_loss: 0.2541 - val_acc: 0.9333\n",
            "Epoch 153/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1927 - acc: 0.9667 - val_loss: 0.2388 - val_acc: 0.9333\n",
            "Epoch 154/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1906 - acc: 0.9667 - val_loss: 0.2543 - val_acc: 0.9333\n",
            "Epoch 155/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1951 - acc: 0.9667 - val_loss: 0.2572 - val_acc: 0.9333\n",
            "Epoch 156/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1900 - acc: 0.9667 - val_loss: 0.2586 - val_acc: 0.9333\n",
            "Epoch 157/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1898 - acc: 0.9583 - val_loss: 0.2537 - val_acc: 0.9333\n",
            "Epoch 158/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1876 - acc: 0.9583 - val_loss: 0.2438 - val_acc: 0.9333\n",
            "Epoch 159/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1870 - acc: 0.9667 - val_loss: 0.2424 - val_acc: 0.9333\n",
            "Epoch 160/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1883 - acc: 0.9667 - val_loss: 0.2388 - val_acc: 0.9333\n",
            "Epoch 161/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1845 - acc: 0.9667 - val_loss: 0.2433 - val_acc: 0.9333\n",
            "Epoch 162/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1864 - acc: 0.9667 - val_loss: 0.2417 - val_acc: 0.9333\n",
            "Epoch 163/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1838 - acc: 0.9667 - val_loss: 0.2413 - val_acc: 0.9333\n",
            "Epoch 164/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1841 - acc: 0.9667 - val_loss: 0.2419 - val_acc: 0.9333\n",
            "Epoch 165/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1828 - acc: 0.9583 - val_loss: 0.2365 - val_acc: 0.9333\n",
            "Epoch 166/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1805 - acc: 0.9667 - val_loss: 0.2363 - val_acc: 0.9333\n",
            "Epoch 167/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1805 - acc: 0.9667 - val_loss: 0.2439 - val_acc: 0.9333\n",
            "Epoch 168/200\n",
            "120/120 [==============================] - 0s 971us/sample - loss: 0.1807 - acc: 0.9583 - val_loss: 0.2370 - val_acc: 0.9333\n",
            "Epoch 169/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1790 - acc: 0.9667 - val_loss: 0.2327 - val_acc: 0.9333\n",
            "Epoch 170/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1789 - acc: 0.9667 - val_loss: 0.2398 - val_acc: 0.9333\n",
            "Epoch 171/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1787 - acc: 0.9583 - val_loss: 0.2219 - val_acc: 0.9667\n",
            "Epoch 172/200\n",
            "120/120 [==============================] - 0s 984us/sample - loss: 0.1764 - acc: 0.9667 - val_loss: 0.2409 - val_acc: 0.9333\n",
            "Epoch 173/200\n",
            "120/120 [==============================] - 0s 969us/sample - loss: 0.1761 - acc: 0.9667 - val_loss: 0.2338 - val_acc: 0.9333\n",
            "Epoch 174/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1754 - acc: 0.9667 - val_loss: 0.2378 - val_acc: 0.9333\n",
            "Epoch 175/200\n",
            "120/120 [==============================] - 0s 980us/sample - loss: 0.1763 - acc: 0.9583 - val_loss: 0.2210 - val_acc: 0.9667\n",
            "Epoch 176/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1775 - acc: 0.9667 - val_loss: 0.2269 - val_acc: 0.9333\n",
            "Epoch 177/200\n",
            "120/120 [==============================] - 0s 977us/sample - loss: 0.1742 - acc: 0.9667 - val_loss: 0.2387 - val_acc: 0.9333\n",
            "Epoch 178/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1730 - acc: 0.9667 - val_loss: 0.2273 - val_acc: 0.9333\n",
            "Epoch 179/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1737 - acc: 0.9667 - val_loss: 0.2303 - val_acc: 0.9333\n",
            "Epoch 180/200\n",
            "120/120 [==============================] - 0s 997us/sample - loss: 0.1723 - acc: 0.9667 - val_loss: 0.2209 - val_acc: 0.9333\n",
            "Epoch 181/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1713 - acc: 0.9667 - val_loss: 0.2257 - val_acc: 0.9333\n",
            "Epoch 182/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1704 - acc: 0.9667 - val_loss: 0.2261 - val_acc: 0.9333\n",
            "Epoch 183/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1717 - acc: 0.9667 - val_loss: 0.2295 - val_acc: 0.9333\n",
            "Epoch 184/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1682 - acc: 0.9750 - val_loss: 0.2169 - val_acc: 0.9333\n",
            "Epoch 185/200\n",
            "120/120 [==============================] - 0s 997us/sample - loss: 0.1685 - acc: 0.9667 - val_loss: 0.2156 - val_acc: 0.9333\n",
            "Epoch 186/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1669 - acc: 0.9667 - val_loss: 0.2218 - val_acc: 0.9333\n",
            "Epoch 187/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1670 - acc: 0.9667 - val_loss: 0.2203 - val_acc: 0.9333\n",
            "Epoch 188/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1673 - acc: 0.9667 - val_loss: 0.2223 - val_acc: 0.9333\n",
            "Epoch 189/200\n",
            "120/120 [==============================] - 0s 978us/sample - loss: 0.1668 - acc: 0.9667 - val_loss: 0.2239 - val_acc: 0.9333\n",
            "Epoch 190/200\n",
            "120/120 [==============================] - 0s 988us/sample - loss: 0.1653 - acc: 0.9667 - val_loss: 0.2202 - val_acc: 0.9333\n",
            "Epoch 191/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1645 - acc: 0.9667 - val_loss: 0.2188 - val_acc: 0.9333\n",
            "Epoch 192/200\n",
            "120/120 [==============================] - 0s 985us/sample - loss: 0.1644 - acc: 0.9667 - val_loss: 0.2109 - val_acc: 0.9667\n",
            "Epoch 193/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1626 - acc: 0.9667 - val_loss: 0.2250 - val_acc: 0.9333\n",
            "Epoch 194/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1636 - acc: 0.9667 - val_loss: 0.2136 - val_acc: 0.9333\n",
            "Epoch 195/200\n",
            "120/120 [==============================] - 0s 976us/sample - loss: 0.1626 - acc: 0.9667 - val_loss: 0.2220 - val_acc: 0.9333\n",
            "Epoch 196/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1608 - acc: 0.9750 - val_loss: 0.2043 - val_acc: 0.9667\n",
            "Epoch 197/200\n",
            "120/120 [==============================] - 0s 939us/sample - loss: 0.1620 - acc: 0.9667 - val_loss: 0.2089 - val_acc: 0.9667\n",
            "Epoch 198/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1602 - acc: 0.9667 - val_loss: 0.2168 - val_acc: 0.9333\n",
            "Epoch 199/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1602 - acc: 0.9667 - val_loss: 0.2147 - val_acc: 0.9333\n",
            "Epoch 200/200\n",
            "120/120 [==============================] - 0s 1ms/sample - loss: 0.1584 - acc: 0.9667 - val_loss: 0.2054 - val_acc: 0.9667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5b_PbcyKyuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}